# Recommend creating a .env file in the root directory for these:
# VAULT_VERSION=1.19-ent
# VAULT_LOG_LEVEL=info
# VAULT_PORT=8200

services:
  vault:
    image: hashicorp/vault-enterprise:${VAULT_VERSION:-1.20-ent}
    container_name: ${COMPOSE_PROJECT_NAME}_vault
    restart: unless-stopped
    ports:
      - "${VAULT_PORT:-8200}:8200"
    env_file:
      - .env # Contains VAULT_ADDR, VAULT_LICENSE
    environment:
      VAULT_LOG_LEVEL: ${VAULT_LOG_LEVEL:-info} # Default to 'info', allow override via .env
    cap_add:
      - IPC_LOCK
    volumes:
      - ./volumes/vault/raft.hcl:/vault/config/raft.hcl:ro
      - vault-data:/vault/file  # Need this volume to fudge permissions error
      - vault-logs:/vault/logs
      - vault-audit-logs:/vault/audit  # Volume for audit logs - accessible to elastic agent
      - ./certs:/vault/certs:ro  # Mount certificates directory
      # Mount individual certificate files for easier access
      - ./certs/ca/ca.crt:/vault/ca.crt:ro
      - ./certs/elasticsearch/elasticsearch.crt:/vault/elasticsearch.crt:ro
      - ./certs/elasticsearch/elasticsearch.key:/vault/elasticsearch.key:ro
      - vault-kerberos-data:/vault/kerberos:ro
    command: server
    depends_on:
      kerberos-keytab-extractor:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8200/v1/sys/health?standbyok=true&sealedcode=204&uninitcode=204"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s # Give Vault time to start before checking   
      
  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-latest}
    container_name: ${COMPOSE_PROJECT_NAME}_grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - ./volumes/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./volumes/grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./volumes/grafana/vault_dashboard.json:/var/lib/grafana/dashboards/vault_dashboard.json:ro
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
    depends_on:
      - prometheus
      - loki
      
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-latest}
    container_name: ${COMPOSE_PROJECT_NAME}_prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./volumes/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  loki:
    image: grafana/loki:${LOKI_VERSION:-latest}
    container_name: ${COMPOSE_PROJECT_NAME}_loki
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - ./volumes/loki/loki.yml:/etc/loki/loki.yml:ro
      - loki-data:/loki/data
      - ./volumes/loki/loki_perm_fix.sh:/loki/loki_perm_fix.sh:ro
    entrypoint: /loki/loki_perm_fix.sh
    user: root # Required for permission fix as Loki runs as user Loki by default

  promtail:
    image: grafana/promtail:${PROMTAIL_VERSION:-latest}
    container_name: ${COMPOSE_PROJECT_NAME}_promtail
    restart: unless-stopped
    volumes:
      - ./volumes/promtail/promtail.yml:/etc/promtail/promtail.yml:ro
      - promtail-data:/promtail
      - vault-logs:/mnt/vault-logs
    command: -config.file=/etc/promtail/promtail.yml
    depends_on:
      - loki

  redis:
    image: redis:latest
    container_name: ${COMPOSE_PROJECT_NAME}_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./config/redis/redis-users.conf:/usr/local/etc/redis/redis-users.conf
    command: ["redis-server", "/usr/local/etc/redis/redis-users.conf"]

  # ELASTIC AND KIBANA - ELK
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: ${COMPOSE_PROJECT_NAME}_elasticsearch
    restart: unless-stopped
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      # Basic settings
      - discovery.type=single-node
      - xpack.security.enabled=true
      - xpack.security.enrollment.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - bootstrap.memory_lock=true
      - ELASTIC_PASSWORD=password123
      
      # TLS Configuration
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/elasticsearch.key
      - xpack.security.http.ssl.certificate=certs/elasticsearch.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca.crt
      - xpack.security.http.ssl.verification_mode=certificate
      
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/elasticsearch.key
      - xpack.security.transport.ssl.certificate=certs/elasticsearch.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      # Mount certificates as read-only
      - ./certs/ca/ca.crt:/usr/share/elasticsearch/config/certs/ca.crt:ro
      - ./certs/ca/ca.key:/usr/share/elasticsearch/config/certs/ca.key:ro
      - ./certs/elasticsearch/elasticsearch.crt:/usr/share/elasticsearch/config/certs/elasticsearch.crt:ro
      - ./certs/elasticsearch/elasticsearch.key:/usr/share/elasticsearch/config/certs/elasticsearch.key:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -s --cacert config/certs/ca.crt -u elastic:password123 https://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # kibana init service - Creates users automatically
  kibana-init:
    image: curlimages/curl:latest
    container_name: ${COMPOSE_PROJECT_NAME}_kibana-init
    depends_on:
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./scripts/elk/10_setup-users.sh:/setup.sh:ro
      - ./certs/ca/ca.crt:/certs/ca.crt:ro
    command: ["/bin/sh", "/setup.sh"]
    restart: "no"          

  # Kibana with TLS (Your original configuration)
  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.0
    container_name: ${COMPOSE_PROJECT_NAME}_kibana
    restart: unless-stopped
    ports:
      - "5601:5601"
    environment:
      # Connect to Elasticsearch via HTTPS
      - ELASTICSEARCH_HOSTS=https://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana_password123
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca.crt
      - ELASTICSEARCH_SSL_VERIFICATIONMODE=certificate
      
      # Enable HTTPS for Kibana
      - SERVER_SSL_ENABLED=true
      - SERVER_SSL_CERTIFICATE=config/certs/kibana.crt
      - SERVER_SSL_KEY=config/certs/kibana.key
      - SERVER_SSL_CERTIFICATEAUTHORITIES=config/certs/ca.crt
      
      # Security settings
      - XPACK_SECURITY_ENABLED=true
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=fhjskloppd678ehkdfdlliverpoolfcr
      - XPACK_REPORTING_ENCRYPTIONKEY=fhjskloppd678ehkdfdlliverpoolfcr
      - XPACK_SECURITY_ENCRYPTIONKEY=fhjskloppd678ehkdfdlliverpoolfcr
    volumes:
      - kibana-data:/usr/share/kibana/data
      # Mount certificates as read-only
      - ./certs/ca/ca.crt:/usr/share/kibana/config/certs/ca.crt:ro
      - ./certs/kibana/kibana.crt:/usr/share/kibana/config/certs/kibana.crt:ro
      - ./certs/kibana/kibana.key:/usr/share/kibana/config/certs/kibana.key:ro    
      # Make sure the users have been created successfully
    depends_on:
      kibana-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -f --cacert config/certs/ca.crt https://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # Fleet initialization service - runs once to set up Fleet
  fleet-init:
    image: curlimages/curl:latest
    container_name: ${COMPOSE_PROJECT_NAME}_fleet_init
    user: root  # Run as root to fix permission issues
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    volumes:
      - ./scripts/elk/fleet/00_init-fleet.sh:/00_init-fleet.sh:ro
      - ./certs/ca/ca.crt:/certs/ca.crt:ro
      - fleet-tokens:/tokens  # Shared volume for tokens
    command: ["/bin/sh", "/00_init-fleet.sh"]
    restart: "no"
    environment:
      - KIBANA_HOST=https://kibana:5601
      - KIBANA_USER=elastic
      - KIBANA_PASSWORD=password123
      - CA_CERT=/certs/ca.crt

  # Fleet Server with automatic token retrieval
  # Note tha the Fleet Server use the same image (elastic-agent) as the elastic-agent - This is because the Fleet Server is essentially an agent which will used to manage other agents. 
  fleet-server:
    image: docker.elastic.co/beats/elastic-agent:8.12.0
    container_name: ${COMPOSE_PROJECT_NAME}_fleet_server
    restart: unless-stopped
    ports:
      - "8220:8220"
    user: root
    depends_on:
      fleet-init:
        condition: service_completed_successfully
    volumes:
      - ./certs/ca/ca.crt:/certs/ca.crt:ro
      - fleet-server-data:/usr/share/elastic-agent/state
      - fleet-tokens:/tokens:ro  # Read tokens from shared volume
      - ./scripts/elk/fleet/10_start-fleet-server.sh:/10_start-fleet-server.sh:ro
    entrypoint: ["/bin/sh", "/10_start-fleet-server.sh"]
    environment:
      - FLEET_SERVER_ENABLE=1
      - FLEET_SERVER_ELASTICSEARCH_HOST=https://elasticsearch:9200
      - FLEET_SERVER_POLICY_ID=fleet-server-policy
      - FLEET_SERVER_HOST=0.0.0.0
      - FLEET_SERVER_PORT=8220
      - FLEET_SERVER_ELASTICSEARCH_USERNAME=elastic
      - FLEET_SERVER_ELASTICSEARCH_PASSWORD=password123
      - FLEET_SERVER_ELASTICSEARCH_CA=/certs/ca.crt
      - FLEET_SERVER_INSECURE_HTTP=true
      - FLEET_URL=http://fleet-server:8220
      - KIBANA_FLEET_HOST=https://kibana:5601
      - KIBANA_FLEET_USERNAME=elastic
      - KIBANA_FLEET_PASSWORD=password123
      - KIBANA_FLEET_CA=/certs/ca.crt

  # Elastic Agent with corrected configuration - enrollment handled by post-deployment script
  elastic-agent:
    image: docker.elastic.co/beats/elastic-agent:8.12.0
    container_name: ${COMPOSE_PROJECT_NAME}_elastic_agent
    restart: unless-stopped
    user: root
    depends_on:
      fleet-server:
        condition: service_started
      vault-permissions:  # Wait for permission fix to complete
        condition: service_completed_successfully
    volumes:
      - elastic-agent-data:/usr/share/elastic-agent/state
      - vault-logs:/mnt/vault-logs:ro         # Vault operational logs
      - fleet-tokens:/tokens:ro
      - ./certs/ca/ca.crt:/certs/ca.crt:ro
    environment:
      - FLEET_URL=http://fleet-server:8220
      - FLEET_INSECURE=true
      - ELASTIC_AGENT_LOG_LEVEL=debug  # ADDED: For better debugging

  # Vault creates audit logs with 600 permissions (owner read/write only)
  # Elastic Agent needs 644 permissions (owner read/write, group/others read) to collect logs
  vault-permissions:
    image: alpine:latest
    container_name: ${COMPOSE_PROJECT_NAME}_vault_permissions
    user: root  # Required to change file permissions
    volumes:
      - vault-logs:/vault-logs  # Mount same volume as Vault to access log files
    command: >
      sh -c "
        echo 'Waiting for Vault to create audit log files...' &&
        sleep 10 &&
        echo 'Fixing permissions on Vault log files...' &&
        chmod -f 644 /vault-logs/*.log 2>/dev/null || true &&
        echo 'Vault log permissions granted - Elastic Agent can now read audit logs'
      "
    restart: "no"  # Run once and exit
    depends_on:
      vault:
        condition: service_started  # Wait for Vault to start creating log files

  kerberos-kdc:
    image: ubuntu:20.04
    container_name: ${COMPOSE_PROJECT_NAME}_kerberos_kdc
    hostname: kdc.example.com
    restart: unless-stopped
    ports:
      - "88:88/tcp"
      - "88:88/udp" 
      - "464:464/tcp"
      - "464:464/udp"
      - "749:749/tcp"
    environment:
      - KRB5_REALM=EXAMPLE.COM
      - KRB5_KDC=kdc.example.com
      - KRB5_PASS=admin123
      - DEBIAN_FRONTEND=noninteractive
    volumes:
      # ONLY mount the script - no config files
      - ./scripts/kerberos/init-kdc.sh:/init-kdc.sh:ro
      - kerberos-data:/var/lib/krb5kdc
    command: ["/init-kdc.sh"]
    # Add healthcheck to ensure KDC is fully ready
    healthcheck:
      test: ["CMD-SHELL", "pgrep krb5kdc && test -f /var/lib/krb5kdc/vault.keytab"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  kerberos-client:
    image: ubuntu:20.04
    container_name: ${COMPOSE_PROJECT_NAME}_kerberos_client
    hostname: client.example.com  # Client hostname in the Kerberos domain
    restart: unless-stopped
    environment:
      - DEBIAN_FRONTEND=noninteractive   # Prevent interactive prompts during package installation
    depends_on:
      - kerberos-kdc  # Wait for KDC to be available before starting
    volumes:
      # Mount the same Kerberos config so client knows how to find the KDC
      - ./scripts/kerberos/krb5.conf:/etc/krb5.conf:ro
      # Mount test scripts for validating Kerberos authentication
      - ./scripts/kerberos/tests/test-auth.sh:/test-auth.sh:ro
      - ./scripts/kerberos/setup-client.sh:/setup-client.sh:ro
    command: ["/setup-client.sh"]  # Install Kerberos client and keep running

  kerberos-keytab-extractor:
    image: alpine:latest
    container_name: ${COMPOSE_PROJECT_NAME}_kerberos_keytab_extractor
    depends_on:
      kerberos-kdc:
        condition: service_healthy  # Wait for KDC to be fully healthy
    volumes:
      - kerberos-data:/kdc-data:ro
      - vault-kerberos-data:/vault-kerberos
      - ./scripts/kerberos/extract-keytab.sh:/extract-keytab.sh:ro
    command: ["sh", "-c", "apk add --no-cache bash && /extract-keytab.sh"]
    restart: "no"

volumes:
  loki-data:
  grafana-data:
  prometheus-data:
  promtail-data:
  vault-data:
  vault-logs:
  vault-audit-logs:  # Shared volume for Vault audit logs between Vault and Elastic Agent
  redis-data:
  elasticsearch-data:
  kibana-data:
  fleet-server-data:
  elastic-agent-data:
  fleet-tokens:  # Shared volume for Fleet tokens
  kerberos-data: # Shared volume for kerberos data
  vault-kerberos-data: